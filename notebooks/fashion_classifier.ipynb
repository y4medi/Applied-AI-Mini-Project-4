{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "276f4e13",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Deep Learning Classifier (COMP 9130 â€“ Mini Project 4)\n",
    "\n",
    "This notebook satisfies the required analyses and deliverables for the StyleSort business case using **PyTorch**:\n",
    "\n",
    "- **Custom `nn.Module` model** (CNN) from `src/model.py`\n",
    "- **Custom training loop** from `src/train.py`\n",
    "- **3+ experiments** (different configs) and comparison\n",
    "- **Training curves** (loss/accuracy)\n",
    "- **Confusion matrix** + business interpretation support\n",
    "- **Cost-weighted accuracy** using a cost matrix\n",
    "- **Confidence-threshold analysis** (accuracy vs. acceptance rate)\n",
    "- **10+ misclassified examples** with confidence scores\n",
    "- Saves figures into `results/`:\n",
    "  - `results/training_curves.png`\n",
    "  - `results/confusion_matrix.png`\n",
    "  - `results/misclassified_examples.png`\n",
    "  - `results/confidence_threshold.png`\n",
    "\n",
    "> Run cells top-to-bottom. If you have a GPU, it will automatically use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523db665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.10.0.dev20251124+cu128\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Project-style folders\n",
    "# Current directory is /notebooks, so go one level up for project root\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(\"..\"))  # one folder up to project root\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, \"results\")  # results/ in project root\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")  # data/ in project root\n",
    "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Add paths for imports\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.insert(0, SRC_PATH)\n",
    "\n",
    "# Import from our modules\n",
    "from model import FashionClassifier\n",
    "from train import train, evaluate\n",
    "from utils import get_device, get_data_loaders, plot_training_curves, plot_confusion_matrix_heatmap\n",
    "\n",
    "device = get_device()\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63358d45",
   "metadata": {},
   "source": [
    "## 1) Data loading (Fashion-MNIST)\n",
    "\n",
    "We use the `get_data_loaders()` function from `utils.py` to load and normalize Fashion-MNIST.\n",
    "- Images are 28Ã—28 grayscale\n",
    "- 10 categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2b722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes (10): ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
      "Training batches: 938\n",
      "Test batches: 157\n"
     ]
    }
   ],
   "source": [
    "# Load data using utility function\n",
    "BATCH_SIZE = 64\n",
    "train_loader, test_loader, classes = get_data_loaders(batch_size=BATCH_SIZE, data_dir=DATA_DIR)\n",
    "\n",
    "num_classes = len(classes)\n",
    "print(f\"Classes ({num_classes}): {classes}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment_section",
   "metadata": {},
   "source": [
    "## 2) Experiment Configuration\n",
    "\n",
    "We'll run 3+ experiments with different configurations:\n",
    "1. **Baseline**: Simple architecture with ReLU\n",
    "2. **Deeper Network**: More hidden layers\n",
    "3. **Batch Normalization**: Add batch norm for better training\n",
    "\n",
    "Each experiment uses the `FashionClassifier` from `model.py` and training functions from `train.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "experiment_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will run 4 experiments for 10 epochs each\n"
     ]
    }
   ],
   "source": [
    "# Define experiment configurations\n",
    "experiments = [\n",
    "    {\n",
    "        'name': 'Baseline (ReLU, 1 hidden layer)',\n",
    "        'hidden_layers': [512],\n",
    "        'activation': 'relu',\n",
    "        'dropout': 0.5,\n",
    "        'use_bn': False,\n",
    "        'optimizer': 'adam',\n",
    "        'lr': 0.001\n",
    "    },\n",
    "    {\n",
    "        'name': 'Deeper Network (2 hidden layers)',\n",
    "        'hidden_layers': [512, 256],\n",
    "        'activation': 'relu',\n",
    "        'dropout': 0.5,\n",
    "        'use_bn': False,\n",
    "        'optimizer': 'adam',\n",
    "        'lr': 0.001\n",
    "    },\n",
    "    {\n",
    "        'name': 'Batch Normalization',\n",
    "        'hidden_layers': [512],\n",
    "        'activation': 'relu',\n",
    "        'dropout': 0.5,\n",
    "        'use_bn': True,\n",
    "        'optimizer': 'adam',\n",
    "        'lr': 0.001\n",
    "    },\n",
    "    {\n",
    "        'name': 'LeakyReLU Activation',\n",
    "        'hidden_layers': [512],\n",
    "        'activation': 'leaky_relu',\n",
    "        'dropout': 0.5,\n",
    "        'use_bn': False,\n",
    "        'optimizer': 'adam',\n",
    "        'lr': 0.001\n",
    "    }\n",
    "]\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "print(f\"Will run {len(experiments)} experiments for {NUM_EPOCHS} epochs each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_section",
   "metadata": {},
   "source": [
    "## 3) Training Loop\n",
    "\n",
    "We train each configuration and track results using functions from `train.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "training_loop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENT 1/4: Baseline (ReLU, 1 hidden layer)\n",
      "================================================================================\n",
      "Config: {'name': 'Baseline (ReLU, 1 hidden layer)', 'hidden_layers': [512], 'activation': 'relu', 'dropout': 0.5, 'use_bn': False, 'optimizer': 'adam', 'lr': 0.001}\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 173.50batch/s, acc=83.8, loss=0.454] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4542 | Train Acc: 83.80%\n",
      "Test Loss:  0.3032 | Test Acc:  88.84%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 175.35batch/s, acc=88.6, loss=0.311] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3113 | Train Acc: 88.65%\n",
      "Test Loss:  0.2760 | Test Acc:  89.64%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 166.91batch/s, acc=90.2, loss=0.267] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2670 | Train Acc: 90.20%\n",
      "Test Loss:  0.2489 | Test Acc:  90.90%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 164.47batch/s, acc=91.4, loss=0.234] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2345 | Train Acc: 91.35%\n",
      "Test Loss:  0.2570 | Test Acc:  90.43%\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 164.76batch/s, acc=92.2, loss=0.212] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2119 | Train Acc: 92.22%\n",
      "Test Loss:  0.2235 | Test Acc:  91.80%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 166.23batch/s, acc=92.9, loss=0.193] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1926 | Train Acc: 92.87%\n",
      "Test Loss:  0.2207 | Test Acc:  91.96%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 165.84batch/s, acc=93.4, loss=0.176] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1757 | Train Acc: 93.42%\n",
      "Test Loss:  0.2376 | Test Acc:  91.82%\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 165.13batch/s, acc=94, loss=0.161]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1609 | Train Acc: 93.97%\n",
      "Test Loss:  0.2304 | Test Acc:  91.64%\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 166.77batch/s, acc=94.6, loss=0.144] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1444 | Train Acc: 94.58%\n",
      "Test Loss:  0.2245 | Test Acc:  92.49%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 163.14batch/s, acc=95, loss=0.134]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1337 | Train Acc: 94.97%\n",
      "Test Loss:  0.2298 | Test Acc:  92.47%\n",
      "\n",
      "âœ“ Experiment complete. Best test accuracy: 92.49%\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 2/4: Deeper Network (2 hidden layers)\n",
      "================================================================================\n",
      "Config: {'name': 'Deeper Network (2 hidden layers)', 'hidden_layers': [512, 256], 'activation': 'relu', 'dropout': 0.5, 'use_bn': False, 'optimizer': 'adam', 'lr': 0.001}\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 162.65batch/s, acc=80.8, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5322 | Train Acc: 80.85%\n",
      "Test Loss:  0.3442 | Test Acc:  87.34%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 157.47batch/s, acc=87.4, loss=0.357] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3566 | Train Acc: 87.39%\n",
      "Test Loss:  0.2911 | Test Acc:  89.56%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 162.55batch/s, acc=89.5, loss=0.299] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2987 | Train Acc: 89.47%\n",
      "Test Loss:  0.2791 | Test Acc:  89.93%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:06<00:00, 155.52batch/s, acc=90.6, loss=0.267] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2665 | Train Acc: 90.58%\n",
      "Test Loss:  0.2484 | Test Acc:  90.91%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 160.56batch/s, acc=91.3, loss=0.243] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2434 | Train Acc: 91.32%\n",
      "Test Loss:  0.2473 | Test Acc:  91.17%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 162.22batch/s, acc=92, loss=0.221]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2214 | Train Acc: 91.99%\n",
      "Test Loss:  0.2345 | Test Acc:  91.72%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 162.97batch/s, acc=92.6, loss=0.203] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2026 | Train Acc: 92.59%\n",
      "Test Loss:  0.2340 | Test Acc:  91.78%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 161.56batch/s, acc=93.1, loss=0.188] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1884 | Train Acc: 93.09%\n",
      "Test Loss:  0.2304 | Test Acc:  91.95%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 167.97batch/s, acc=93.6, loss=0.176] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1759 | Train Acc: 93.57%\n",
      "Test Loss:  0.2240 | Test Acc:  91.95%\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:06<00:00, 142.88batch/s, acc=94.2, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1629 | Train Acc: 94.17%\n",
      "Test Loss:  0.2431 | Test Acc:  91.94%\n",
      "\n",
      "âœ“ Experiment complete. Best test accuracy: 91.95%\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 3/4: Batch Normalization\n",
      "================================================================================\n",
      "Config: {'name': 'Batch Normalization', 'hidden_layers': [512], 'activation': 'relu', 'dropout': 0.5, 'use_bn': True, 'optimizer': 'adam', 'lr': 0.001}\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 174.92batch/s, acc=87.3, loss=0.351] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3510 | Train Acc: 87.29%\n",
      "Test Loss:  0.2715 | Test Acc:  89.82%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 180.10batch/s, acc=90.9, loss=0.248] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2484 | Train Acc: 90.92%\n",
      "Test Loss:  0.2629 | Test Acc:  90.38%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 173.04batch/s, acc=92.2, loss=0.211] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2106 | Train Acc: 92.24%\n",
      "Test Loss:  0.2373 | Test Acc:  91.41%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 183.15batch/s, acc=93.3, loss=0.182] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1825 | Train Acc: 93.25%\n",
      "Test Loss:  0.2128 | Test Acc:  92.61%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 160.56batch/s, acc=94.1, loss=0.161] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1611 | Train Acc: 94.10%\n",
      "Test Loss:  0.2142 | Test Acc:  92.24%\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:06<00:00, 149.36batch/s, acc=94.8, loss=0.14]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1404 | Train Acc: 94.81%\n",
      "Test Loss:  0.2213 | Test Acc:  92.64%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:06<00:00, 145.55batch/s, acc=95.3, loss=0.127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1274 | Train Acc: 95.27%\n",
      "Test Loss:  0.2291 | Test Acc:  92.37%\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:06<00:00, 151.59batch/s, acc=95.8, loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1112 | Train Acc: 95.84%\n",
      "Test Loss:  0.2344 | Test Acc:  92.69%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 162.11batch/s, acc=96.5, loss=0.0948] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0948 | Train Acc: 96.50%\n",
      "Test Loss:  0.2485 | Test Acc:  92.12%\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 161.10batch/s, acc=96.9, loss=0.0854] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0854 | Train Acc: 96.92%\n",
      "Test Loss:  0.2473 | Test Acc:  92.41%\n",
      "\n",
      "âœ“ Experiment complete. Best test accuracy: 92.69%\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 4/4: LeakyReLU Activation\n",
      "================================================================================\n",
      "Config: {'name': 'LeakyReLU Activation', 'hidden_layers': [512], 'activation': 'leaky_relu', 'dropout': 0.5, 'use_bn': False, 'optimizer': 'adam', 'lr': 0.001}\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 165.58batch/s, acc=84.1, loss=0.443] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4435 | Train Acc: 84.08%\n",
      "Test Loss:  0.3309 | Test Acc:  87.98%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 165.78batch/s, acc=88.8, loss=0.305] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3048 | Train Acc: 88.84%\n",
      "Test Loss:  0.2764 | Test Acc:  89.79%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 170.41batch/s, acc=90.5, loss=0.259] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2587 | Train Acc: 90.52%\n",
      "Test Loss:  0.2497 | Test Acc:  90.73%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:04<00:00, 188.26batch/s, acc=91.7, loss=0.227] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2274 | Train Acc: 91.68%\n",
      "Test Loss:  0.2357 | Test Acc:  91.57%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:04<00:00, 190.93batch/s, acc=92.5, loss=0.203] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2033 | Train Acc: 92.51%\n",
      "Test Loss:  0.2244 | Test Acc:  91.67%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 186.45batch/s, acc=93.1, loss=0.187] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1869 | Train Acc: 93.14%\n",
      "Test Loss:  0.2494 | Test Acc:  91.69%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:04<00:00, 188.88batch/s, acc=93.8, loss=0.167] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1668 | Train Acc: 93.80%\n",
      "Test Loss:  0.2236 | Test Acc:  92.15%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:04<00:00, 188.24batch/s, acc=94.2, loss=0.151] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1513 | Train Acc: 94.23%\n",
      "Test Loss:  0.2181 | Test Acc:  92.60%\n",
      "âœ“ New best model!\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 169.72batch/s, acc=95, loss=0.135]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1350 | Train Acc: 95.04%\n",
      "Test Loss:  0.2281 | Test Acc:  92.41%\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:05<00:00, 159.45batch/s, acc=95.5, loss=0.121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1211 | Train Acc: 95.53%\n",
      "Test Loss:  0.2242 | Test Acc:  92.62%\n",
      "âœ“ New best model!\n",
      "\n",
      "âœ“ Experiment complete. Best test accuracy: 92.62%\n",
      "\n",
      "================================================================================\n",
      "ALL EXPERIMENTS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Store results for all experiments\n",
    "all_results = []\n",
    "\n",
    "for exp_idx, config in enumerate(experiments):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPERIMENT {exp_idx + 1}/{len(experiments)}: {config['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Config: {config}\")\n",
    "    \n",
    "    # Create model using FashionClassifier from model.py\n",
    "    model = FashionClassifier(\n",
    "        num_classes=num_classes,\n",
    "        hidden_layers=config['hidden_layers'],\n",
    "        activation=config['activation'],\n",
    "        dropout_rate=config['dropout'],\n",
    "        use_bn=config['use_bn']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    if config['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    elif config['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9)\n",
    "    elif config['optimizer'] == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    # Training history\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accs, test_accs = [], []\n",
    "    best_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training loop using train() and evaluate() from train.py\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        # Train for one epoch\n",
    "        train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_acc, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Test Loss:  {test_loss:.4f} | Test Acc:  {test_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(\"âœ“ New best model!\")\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'name': config['name'],\n",
    "        'config': config,\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'test_accs': test_accs,\n",
    "        'best_acc': best_acc,\n",
    "        'best_model_state': best_model_state\n",
    "    }\n",
    "    all_results.append(result)\n",
    "    \n",
    "    print(f\"\\nâœ“ Experiment complete. Best test accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_comparison",
   "metadata": {},
   "source": [
    "## 4) Compare Experiment Results\n",
    "\n",
    "We compare all experiments and select the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "compare_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Comparison:\n",
      "================================================================================\n",
      "Experiment                               Best Test Acc  \n",
      "================================================================================\n",
      "Baseline (ReLU, 1 hidden layer)                 92.49%\n",
      "Deeper Network (2 hidden layers)                91.95%\n",
      "Batch Normalization                             92.69%\n",
      "LeakyReLU Activation                            92.62%\n",
      "================================================================================\n",
      "\n",
      "ðŸ† BEST MODEL: Batch Normalization\n",
      "   Accuracy: 92.69%\n",
      "   Meets >85% requirement: âœ“ YES\n"
     ]
    }
   ],
   "source": [
    "# Print comparison table\n",
    "print(\"\\nExperiment Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Experiment':<40} {'Best Test Acc':<15}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in all_results:\n",
    "    print(f\"{result['name']:<40} {result['best_acc']:>12.2f}%\")\n",
    "\n",
    "# Find best experiment\n",
    "best_exp_idx = np.argmax([r['best_acc'] for r in all_results])\n",
    "best_experiment = all_results[best_exp_idx]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nðŸ† BEST MODEL: {best_experiment['name']}\")\n",
    "print(f\"   Accuracy: {best_experiment['best_acc']:.2f}%\")\n",
    "print(f\"   Meets >85% requirement: {'âœ“ YES' if best_experiment['best_acc'] > 85 else 'âœ— NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot_training",
   "metadata": {},
   "source": [
    "## 5) Plot Training Curves\n",
    "\n",
    "Using `plot_training_curves()` from `utils.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "plot_curves",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training curves saved to c:\\Users\\Sepeh\\source\\repos\\Applied-AI-Mini-Project-4\\results\\training_curves.png\n",
      "âœ“ Training curves saved to c:\\Users\\Sepeh\\source\\repos\\Applied-AI-Mini-Project-4\\results/training_curves.png\n"
     ]
    }
   ],
   "source": [
    "# Plot training curves for best experiment using utils.py function\n",
    "plot_training_curves(\n",
    "    train_losses=best_experiment['train_losses'],\n",
    "    test_losses=best_experiment['test_losses'],\n",
    "    train_accs=best_experiment['train_accs'],\n",
    "    test_accs=best_experiment['test_accs'],\n",
    "    save_path=os.path.join(RESULTS_DIR, 'training_curves.png')\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Training curves saved to {RESULTS_DIR}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_best",
   "metadata": {},
   "source": [
    "## 6) Load Best Model for Analysis\n",
    "\n",
    "We'll reload the best model and get predictions with probabilities for detailed analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "load_best_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Best model loaded\n",
      "âœ“ Collected 10000 predictions\n"
     ]
    }
   ],
   "source": [
    "# Create model with best configuration\n",
    "best_config = best_experiment['config']\n",
    "best_model = FashionClassifier(\n",
    "    num_classes=num_classes,\n",
    "    hidden_layers=best_config['hidden_layers'],\n",
    "    activation=best_config['activation'],\n",
    "    dropout_rate=best_config['dropout'],\n",
    "    use_bn=best_config['use_bn']\n",
    ").to(device)\n",
    "\n",
    "# Load best weights\n",
    "best_model.load_state_dict(best_experiment['best_model_state'])\n",
    "print(\"âœ“ Best model loaded\")\n",
    "\n",
    "# Get predictions with probabilities for analysis\n",
    "@torch.no_grad()\n",
    "def get_predictions_with_probs(model, loader, device):\n",
    "    \"\"\"Get predictions and probabilities for entire dataset.\"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = probs.argmax(dim=1)\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_labels), np.array(all_preds), np.array(all_probs)\n",
    "\n",
    "y_true, y_pred, y_prob = get_predictions_with_probs(best_model, test_loader, device)\n",
    "print(f\"âœ“ Collected {len(y_true)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion_matrix",
   "metadata": {},
   "source": [
    "## 7) Confusion Matrix\n",
    "\n",
    "Using `plot_confusion_matrix_heatmap()` from `utils.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "plot_confusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix saved to c:\\Users\\Sepeh\\source\\repos\\Applied-AI-Mini-Project-4\\results\\confusion_matrix.png\n",
      "âœ“ Confusion matrix saved to c:\\Users\\Sepeh\\source\\repos\\Applied-AI-Mini-Project-4\\results/confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "# Plot confusion matrix using utils.py function\n",
    "plot_confusion_matrix_heatmap(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    "    classes=classes,\n",
    "    save_path=os.path.join(RESULTS_DIR, 'confusion_matrix.png')\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Confusion matrix saved to {RESULTS_DIR}/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost_analysis",
   "metadata": {},
   "source": [
    "## 8) Business Cost Analysis\n",
    "\n",
    "StyleSort has different costs for different types of misclassifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cost_matrix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total misclassification cost: 1442\n",
      "Average cost per prediction: 0.144\n",
      "\n",
      "Top 5 most costly confusions:\n",
      "1.           Shirt â†’ T-shirt/top     | Count: 132 | Cost:    396\n",
      "2.            Coat â†’ Pullover        | Count:  63 | Cost:    189\n",
      "3.     T-shirt/top â†’ Shirt           | Count:  54 | Cost:    162\n",
      "4.      Ankle boot â†’ Sneaker         | Count:  47 | Cost:    118\n",
      "5.        Pullover â†’ Coat            | Count:  33 | Cost:     99\n"
     ]
    }
   ],
   "source": [
    "# Define cost matrix (cost of predicting column when truth is row)\n",
    "# Higher costs for confusions that lead to customer dissatisfaction\n",
    "cost_matrix = np.ones((num_classes, num_classes))\n",
    "np.fill_diagonal(cost_matrix, 0)  # No cost for correct predictions\n",
    "\n",
    "# High-cost confusions (as per business case)\n",
    "# Shirt (6) <-> T-shirt/top (0): formal vs casual expectations\n",
    "cost_matrix[6, 0] = 3  # Shirt predicted as T-shirt\n",
    "cost_matrix[0, 6] = 3  # T-shirt predicted as Shirt\n",
    "\n",
    "# Coat (4) <-> Pullover (2): warmth/size expectations\n",
    "cost_matrix[4, 2] = 3  # Coat predicted as Pullover\n",
    "cost_matrix[2, 4] = 3  # Pullover predicted as Coat\n",
    "\n",
    "# Sneaker (7) <-> Ankle boot (9): price point differences\n",
    "cost_matrix[7, 9] = 2.5\n",
    "cost_matrix[9, 7] = 2.5\n",
    "\n",
    "# Sandal (5) <-> Ankle boot (9): completely different\n",
    "cost_matrix[5, 9] = 4\n",
    "cost_matrix[9, 5] = 4\n",
    "\n",
    "# Calculate weighted cost\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "total_cost = np.sum(cm * cost_matrix)\n",
    "avg_cost_per_prediction = total_cost / len(y_true)\n",
    "\n",
    "print(f\"Total misclassification cost: {total_cost:.0f}\")\n",
    "print(f\"Average cost per prediction: {avg_cost_per_prediction:.3f}\")\n",
    "print(f\"\\nTop 5 most costly confusions:\")\n",
    "\n",
    "# Finding most costly confusions\n",
    "confusion_costs = cm * cost_matrix\n",
    "np.fill_diagonal(confusion_costs, 0)  # Ignore correct predictions\n",
    "\n",
    "# Get top confusions\n",
    "top_confusions = []\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        if i != j and confusion_costs[i, j] > 0:\n",
    "            top_confusions.append((confusion_costs[i, j], i, j, cm[i, j]))\n",
    "\n",
    "top_confusions.sort(reverse=True)\n",
    "\n",
    "for idx, (cost, true_idx, pred_idx, count) in enumerate(top_confusions[:5], 1):\n",
    "    print(f\"{idx}. {classes[true_idx]:>15} â†’ {classes[pred_idx]:<15} | Count: {count:>3} | Cost: {cost:>6.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidence_threshold",
   "metadata": {},
   "source": [
    "## 9) Confidence Threshold Analysis\n",
    "\n",
    "StyleSort wants to route low-confidence predictions to human review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "confidence_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Confidence analysis saved to c:\\Users\\Sepeh\\source\\repos\\Applied-AI-Mini-Project-4\\results\\confidence_threshold.png\n",
      "\n",
      "================================================================================\n",
      "CONFIDENCE THRESHOLD RECOMMENDATIONS\n",
      "================================================================================\n",
      "Threshold 0.50: 92.83% accuracy on  99.1% of predictions\n",
      "Threshold 0.55: 93.45% accuracy on  98.0% of predictions\n",
      "Threshold 0.60: 93.83% accuracy on  97.0% of predictions\n",
      "Threshold 0.65: 94.34% accuracy on  96.0% of predictions\n",
      "Threshold 0.70: 94.86% accuracy on  94.8% of predictions\n",
      "Threshold 0.75: 95.54% accuracy on  93.3% of predictions\n",
      "Threshold 0.80: 96.04% accuracy on  91.7% of predictions\n",
      "Threshold 0.85: 96.60% accuracy on  90.0% of predictions\n",
      "Threshold 0.90: 97.11% accuracy on  88.0% of predictions\n",
      "Threshold 0.95: 97.89% accuracy on  84.5% of predictions\n",
      "\n",
      "- RECOMMENDED: Threshold 0.95\n",
      "   â†’ 97.89% accuracy\n",
      "   â†’ 84.5% auto-classified\n",
      "   â†’ 15.5% sent to human review\n"
     ]
    }
   ],
   "source": [
    "# Analyze accuracy vs confidence threshold\n",
    "max_probs = y_prob.max(axis=1)  # Maximum probability for each prediction\n",
    "\n",
    "thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "accuracies = []\n",
    "acceptance_rates = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Only consider predictions above threshold\n",
    "    mask = max_probs >= threshold\n",
    "    if mask.sum() > 0:\n",
    "        acc = (y_true[mask] == y_pred[mask]).mean() * 100\n",
    "        acceptance = mask.mean() * 100\n",
    "    else:\n",
    "        acc = 0\n",
    "        acceptance = 0\n",
    "    \n",
    "    accuracies.append(acc)\n",
    "    acceptance_rates.append(acceptance)\n",
    "\n",
    "# Plot confidence threshold analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy vs Threshold\n",
    "ax1.plot(thresholds, accuracies, marker='o', linewidth=2)\n",
    "ax1.axhline(y=85, color='r', linestyle='--', label='85% Target')\n",
    "ax1.set_xlabel('Confidence Threshold', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('Accuracy vs Confidence Threshold', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Acceptance Rate vs Threshold\n",
    "ax2.plot(thresholds, acceptance_rates, marker='s', linewidth=2, color='green')\n",
    "ax2.set_xlabel('Confidence Threshold', fontsize=12)\n",
    "ax2.set_ylabel('Acceptance Rate (%)', fontsize=12)\n",
    "ax2.set_title('Auto-Classification Rate vs Threshold', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "conf_path = os.path.join(RESULTS_DIR, 'confidence_threshold.png')\n",
    "plt.savefig(conf_path, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Confidence analysis saved to {conf_path}\")\n",
    "\n",
    "# Print recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIDENCE THRESHOLD RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "for threshold, acc, accept in zip(thresholds, accuracies, acceptance_rates):\n",
    "    print(f\"Threshold {threshold:.2f}: {acc:5.2f}% accuracy on {accept:5.1f}% of predictions\")\n",
    "\n",
    "# Suggest optimal threshold\n",
    "optimal_idx = np.argmax([a if r > 80 else 0 for a, r in zip(accuracies, acceptance_rates)])\n",
    "print(f\"\\n- RECOMMENDED: Threshold {thresholds[optimal_idx]:.2f}\")\n",
    "print(f\"   â†’ {accuracies[optimal_idx]:.2f}% accuracy\")\n",
    "print(f\"   â†’ {acceptance_rates[optimal_idx]:.1f}% auto-classified\")\n",
    "print(f\"   â†’ {100 - acceptance_rates[optimal_idx]:.1f}% sent to human review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "misclassified",
   "metadata": {},
   "source": [
    "## 10) Visualize Misclassified Examples\n",
    "\n",
    "Show 12 misclassified examples with confidence scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "viz_misclassified",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified count: 759 out of 10000 (7.6%)\n",
      "âœ“ Misclassified examples saved to c:\\Users\\Sepeh\\source\\repos\\Applied-AI-Mini-Project-4\\results\\misclassified_examples.png\n"
     ]
    }
   ],
   "source": [
    "# Find misclassified examples\n",
    "mis_idx = np.where(y_true != y_pred)[0]\n",
    "print(f\"Misclassified count: {len(mis_idx)} out of {len(y_true)} ({len(mis_idx)/len(y_true)*100:.1f}%)\")\n",
    "\n",
    "# Randomly select 12 examples\n",
    "k = min(12, len(mis_idx))\n",
    "np.random.shuffle(mis_idx)\n",
    "chosen = mis_idx[:k]\n",
    "\n",
    "# Load test dataset without normalization for visualization\n",
    "viz_transform = transforms.ToTensor()\n",
    "test_dataset_viz = datasets.FashionMNIST(\n",
    "    root=DATA_DIR, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=viz_transform\n",
    ")\n",
    "\n",
    "# Plot misclassified examples\n",
    "cols = 6\n",
    "rows = math.ceil(k / cols)\n",
    "plt.figure(figsize=(14, 4.5))\n",
    "\n",
    "for i, idx in enumerate(chosen, start=1):\n",
    "    img, true_label = test_dataset_viz[idx]\n",
    "    pred_label = int(y_pred[idx])\n",
    "    conf = float(y_prob[idx].max())\n",
    "\n",
    "    plt.subplot(rows, cols, i)\n",
    "    plt.imshow(img.squeeze().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\n",
    "        f\"True: {classes[true_label]}\\nPred: {classes[pred_label]}\\nConf: {conf:.2f}\", \n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "plt.suptitle('Misclassified Examples', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "mis_path = os.path.join(RESULTS_DIR, 'misclassified_examples.png')\n",
    "plt.savefig(mis_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Misclassified examples saved to {mis_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_speed",
   "metadata": {},
   "source": [
    "## 11) Inference Speed Analysis\n",
    "\n",
    "StyleSort needs to process 10,000 listings/day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "measure_speed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INFERENCE SPEED ANALYSIS\n",
      "================================================================================\n",
      "Average batch inference time: 0.0004 sec/batch\n",
      "Approximate images/sec: 143751.3\n",
      "\n",
      "Business requirement: 10,000 images/day\n",
      "  â†’ Required speed: 0.116 images/sec\n",
      "  â†’ Current speed: 143751.3 images/sec\n",
      "  â†’ Margin: 1242011.4x faster than required\n",
      "\n",
      "âœ“ MEETS SPEED REQUIREMENT: YES\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def measure_inference_time(model, loader, device, warmup_batches=5, measure_batches=50):\n",
    "    \"\"\"Measure inference speed.\"\"\"\n",
    "    model.eval()\n",
    "    it = iter(loader)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(warmup_batches):\n",
    "        images, _ = next(it)\n",
    "        _ = model(images.to(device))\n",
    "\n",
    "    # Measure\n",
    "    times = []\n",
    "    it = iter(loader)\n",
    "    for _ in range(measure_batches):\n",
    "        images, _ = next(it)\n",
    "        images = images.to(device)\n",
    "\n",
    "        t0 = time.time()\n",
    "        _ = model(images)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        times.append(time.time() - t0)\n",
    "\n",
    "    times = np.array(times)\n",
    "    avg = times.mean()\n",
    "    batch_size = next(iter(loader))[0].shape[0]\n",
    "    imgs_per_sec = batch_size / avg\n",
    "    return avg, imgs_per_sec\n",
    "\n",
    "avg_batch_sec, ips = measure_inference_time(best_model, test_loader, device)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INFERENCE SPEED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Average batch inference time: {avg_batch_sec:.4f} sec/batch\")\n",
    "print(f\"Approximate images/sec: {ips:.1f}\")\n",
    "print(f\"\\nBusiness requirement: 10,000 images/day\")\n",
    "print(f\"  â†’ Required speed: {10000/86400:.3f} images/sec\")\n",
    "print(f\"  â†’ Current speed: {ips:.1f} images/sec\")\n",
    "print(f\"  â†’ Margin: {ips/(10000/86400):.1f}x faster than required\")\n",
    "print(f\"\\nâœ“ MEETS SPEED REQUIREMENT: {'YES' if ips > 10000/86400 else 'NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "business_recommendations",
   "metadata": {},
   "source": [
    "## 12) Business Recommendations Summary\n",
    "\n",
    "Based on the analysis above, here are recommendations for StyleSort:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "final_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STYLESORT BUSINESS RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "1. MODEL PERFORMANCE\n",
      "   â€¢ Achieved 92.69% accuracy\n",
      "   â€¢ âœ“ Meets 85% business requirement\n",
      "   â€¢ Best configuration: Batch Normalization\n",
      "\n",
      "2. COST REDUCTION STRATEGY\n",
      "   â€¢ Average misclassification cost: 0.144 per prediction\n",
      "   â€¢ Focus quality improvements on:\n",
      "     1. Shirt â†” T-shirt/top (132 occurrences)\n",
      "     2. Coat â†” Pullover (63 occurrences)\n",
      "     3. T-shirt/top â†” Shirt (54 occurrences)\n",
      "\n",
      "3. HUMAN-IN-THE-LOOP RECOMMENDATION\n",
      "   â€¢ Implement confidence threshold of 0.95\n",
      "   â€¢ Auto-classify 84.5% of products\n",
      "   â€¢ Route 15.5% to human review\n",
      "   â€¢ Achieves 97.89% accuracy on auto-classified items\n",
      "\n",
      "4. OPERATIONAL FEASIBILITY\n",
      "   â€¢ Can process 143751.3 images/second\n",
      "   â€¢ 1242011.4x faster than 10,000/day requirement\n",
      "   â€¢ âœ“ Real-time processing is feasible\n",
      "\n",
      "5. NEXT STEPS\n",
      "   â€¢ Improve product photography for frequently confused categories\n",
      "   â€¢ Add metadata/tags for high-cost misclassification pairs\n",
      "   â€¢ Implement confidence-based routing to human reviewers\n",
      "   â€¢ Monitor real-world performance and retrain quarterly\n",
      "   â€¢ Consider data augmentation to reduce specific confusions\n",
      "\n",
      "================================================================================\n",
      "âœ“ ANALYSIS COMPLETE - All deliverables saved to results/ folder\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STYLESORT BUSINESS RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. MODEL PERFORMANCE\")\n",
    "print(f\"   â€¢ Achieved {best_experiment['best_acc']:.2f}% accuracy\")\n",
    "print(f\"   â€¢ {'âœ“' if best_experiment['best_acc'] > 85 else 'âœ—'} Meets 85% business requirement\")\n",
    "print(f\"   â€¢ Best configuration: {best_experiment['name']}\")\n",
    "\n",
    "print(\"\\n2. COST REDUCTION STRATEGY\")\n",
    "print(f\"   â€¢ Average misclassification cost: {avg_cost_per_prediction:.3f} per prediction\")\n",
    "print(\"   â€¢ Focus quality improvements on:\")\n",
    "for idx, (cost, true_idx, pred_idx, count) in enumerate(top_confusions[:3], 1):\n",
    "    print(f\"     {idx}. {classes[true_idx]} â†” {classes[pred_idx]} ({count} occurrences)\")\n",
    "\n",
    "print(\"\\n3. HUMAN-IN-THE-LOOP RECOMMENDATION\")\n",
    "print(f\"   â€¢ Implement confidence threshold of {thresholds[optimal_idx]:.2f}\")\n",
    "print(f\"   â€¢ Auto-classify {acceptance_rates[optimal_idx]:.1f}% of products\")\n",
    "print(f\"   â€¢ Route {100 - acceptance_rates[optimal_idx]:.1f}% to human review\")\n",
    "print(f\"   â€¢ Achieves {accuracies[optimal_idx]:.2f}% accuracy on auto-classified items\")\n",
    "\n",
    "print(\"\\n4. OPERATIONAL FEASIBILITY\")\n",
    "print(f\"   â€¢ Can process {ips:.1f} images/second\")\n",
    "print(f\"   â€¢ {ips/(10000/86400):.1f}x faster than 10,000/day requirement\")\n",
    "print(\"   â€¢ âœ“ Real-time processing is feasible\")\n",
    "\n",
    "print(\"\\n5. NEXT STEPS\")\n",
    "print(\"   â€¢ Improve product photography for frequently confused categories\")\n",
    "print(\"   â€¢ Add metadata/tags for high-cost misclassification pairs\")\n",
    "print(\"   â€¢ Implement confidence-based routing to human reviewers\")\n",
    "print(\"   â€¢ Monitor real-world performance and retrain quarterly\")\n",
    "print(\"   â€¢ Consider data augmentation to reduce specific confusions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ“ ANALYSIS COMPLETE - All deliverables saved to results/ folder\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389737cc",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "#### 1. Confusion Matrix Analysis\n",
    "\n",
    "The confusion matrix reveals that the most problematic category pairs are Shirt and T-shirt/top (164 total confusions) and Coat and Pullover (89 total confusions). These errors make intuitive sense given the visual similarity at 28x28 resolution. Shirts and t-shirts share similar upper-body silhouettes, while coats and pullovers are both long-sleeved garments where distinguishing features like zippers and material weight are difficult to discern in grayscale. Footwear categories perform well, with Sandals (979/1000) and Sneakers (986/1000) achieving high accuracy due to their distinctive shapes.\n",
    "\n",
    "StyleSort should implement enhanced photography guidelines for Shirt/T-shirt categories ensuring collars and buttons are clearly visible. Human review should be required for high-confusion pairs when confidence falls below 90%. Mandatory metadata fields (weight, closure type, formality) should be added for Coat/Pullover listings to validate predictions. Standardized photography setups with consistent angles and lighting would help highlight category-distinguishing features.\n",
    "\n",
    "#### 2. Error Cost Analysis\n",
    "\n",
    "The cost matrix assigns different penalties based on business impact. High-cost confusions (cost of 3) include Shirt and T-shirt/top misclassifications due to style expectations, and Coat and Pullover confusions due to warmth and sizing differences. Medium-cost confusions (cost of 2.5) involve Sneaker and Ankle boot due to price differences. Sandal and Ankle boot carry the highest cost (4) as completely different footwear types. Other misclassifications have a baseline cost of 1. The total misclassification cost across 10,000 test predictions is 1,314, giving an average cost of 0.131 per prediction. This weighted metric shows business impact better than standard accuracy since it accounts for which errors matter most to customers.\n",
    "\n",
    "The top five costly confusions show where StyleSort faces the most risk. Shirt misclassified as T-shirt/top (86 instances, cost 258) and T-shirt/top misclassified as Shirt (78 instances, cost 234) together make up 37% of total cost. Coat and Pullover confusions add another 267 cost units (55 and 34 instances). Ankle boot confused with Sneaker (34 instances, cost 85) completes the top five.\n",
    "\n",
    "While the model gets 92.98% accuracy overall, the business-weighted view shows errors concentrate in the most expensive categories. Just the Shirt and T-shirt confusion creates almost 40% of total business cost. This supports focusing photography improvements and human review on these specific high-cost pairs, since fixing these errors would have the biggest impact on reducing StyleSort's return rate.\n",
    "\n",
    "#### 3. Confidence Threshold Analysis\n",
    "\n",
    "The optimal confidence threshold for StyleSort is 95%, where the model achieves 98.05% accuracy on auto-classified items while routing 15.6% of products to human review. This balance significantly improves upon the baseline 92.98% accuracy (reducing errors from 7% to 2%) while keeping human review volume manageable at approximately 520 items per day out of 10,000 daily listings.\n",
    "\n",
    "At 80% confidence, the model auto-classifies 97% of items with 96% accuracy, requiring only 3% human review but with more errors. At 99% confidence, accuracy reaches 98% but 16% need review, nearly doubling the workload for minimal accuracy gain. The 95% threshold provides the best operational balance, exceeding the 85% business requirement while maintaining efficient automation.\n",
    "\n",
    "#### 4. Misclassified Examples Analysis\n",
    "\n",
    "The misclassification examples reveal systematic patterns explaining model errors. Upper-body garments show the highest confusion. A T-shirt/top classified as Shirt with 100% confidence indicates the model struggles to distinguish collars and buttons at 28x28 resolution. A Sandal misclassified as Sneaker with 100% confidence suggests photographing angle or grayscale rendering eliminated distinguishing features, showing that high confidence does not guarantee correctness. Several errors show low confidence (37%, 53%, 58%), which would be caught by the recommended 95% threshold and routed to human review. However, high-confidence errors (94%, 99%, 100%) reveal fundamental limitations. The model confidently mistakes visual similarity for category membership when fine details are imperceptible at low resolution.\n",
    "\n",
    "These mistakes occur because the 28x28 grayscale format removes critical details like button types, fabric textures, and material weights that humans use for classification. When garments share similar silhouettes (shirts/pullovers/coats), the model relies on overall shape patterns that can be misleading. Dresses misclassified as upper-body garments suggest the model over-weights torso features and under-weights garment length. Product photography should be standardized with category-specific guidelines highlighting distinguishing features. The 95% confidence threshold should be implemented to catch uncertain predictions. Multi-angle image inputs (front, back, detail) could provide additional visual context for better category distinction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36861ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
